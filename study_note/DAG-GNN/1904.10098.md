# DAG-GNN: DAG Structure Learning with Graph Neural Networks

Author: Yue Yu, Jie Chen, Tian Gao, Mo Yu  
Submitted on 22 Apr 2019  
Cite : [arXiv:1904.10098](https://arxiv.org/pdf/1904.10098.pdf),
Code : [GitHub](https://github.com/fishmoon1234/DAG-GNN)
***

![category](https://img.shields.io/badge/category-paper-00a0a0.svg?longCache=true)
![topic](https://img.shields.io/badge/topic-causal_analysis-a000a0.svg?longCache=true)
![progress](https://progress-bar.dev/15/?title=progress)

<script type="javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            displayMath: [['$$','$$'], ["\\[","\\]"]]
        }
    })
</script>

## 要約

DAGの構造学習にGNNを取り入れてみた。
前提になっているのは、DAGs with NO TEARS(causalnexで使われている方法)のようで、それで微分可能な連続関数による非循環条件の要求ができるようになったので、それに触発されて作ったようだ。
Gitにもあがっているデモンストレーションでは、NO TEARSを上回るような性能が出せる他、非線形なSEMや離散値も扱えるなど対応範囲の面で特に強みを見せている。

## 逐語訳

### 概要

- 信頼性の高いDAGの学習はチャレンジングな課題である(ノード増加に伴って探索空間が爆発的に増えるため)
- 最近は非循環の条件を微分可能な最小化問題に落とし込むというブレイクスルーがあった
- ただ、彼らは線形の構造方程式(SEM)を仮定して、最小二乗法での損失関数を使っており、よく理解されている反面、制限の多い手法を使っていた。
- DLが複雑な非線形マッピングに対して幅広く応用されてきていることを受けて、DLを利用した構造的な制約を加えたDAGの学習方法を提案する。
- これはGNNを利用したVariational AutoEncoder(VAE)を使った手法になっており、DAG-GNNと名付けている。
- さらにより用途を拡張するために、ベクトル値と同様に、離散値を自然に扱えるようにした。
- 合成データを使ったデモでは、非線形を仮定して作られたサンプルに対しては、より正確な構造学習ができることを示した。
- 離散値を用いたデータでは、大域的最適化と遜色のない結果を示すことを確認した。

## 導入

ベイジアンネットワーク(BN)は機械学習分野で広く利用されてきた。
BNは有向非循環グラフ(DAG)の形式をとり、因果効果の中心的な役割を果たしている。
構造学習の課題はNP困難であり、組み合わせの爆発的増加が挙げられる。

スコアベースの手法は、不明な係数行列(A)に対するスコア関数を最適化するもので、グラフが非循環になるという制約が課されるものになる。
最適化の実質的な課題は、複雑な探索領域である。
そのため、実質的には、さらに構造上の仮定が必要になる。

近年、非循環の制約と等価な連続的な関数が導入された。
これにより、組み合わせ問題が連続値の最適化に劇的に変化し、しっかりしたブラックボックスな処理によって効率的に処理できる解決できる可能性が出てきた。
この最適化問題は、非線形なこともあるが、一般にグローバルな最適化というよりはある終端点に落ち着くようなものになっている。
とはいえ、そのようなローカルの解法が組み合わせ問題を時に行くようなグローバルな解法と同等な精度を発揮することも示している。

制限の再定式化にインスパイアされて、目的関数を再考した。
スコアベースの目的関数は、一般的に変数とモデルクラスの仮定を作ってしまう。
例えば、Zhengの論文では、線形のSEMを最小二乗法で最適化する方法をとっていた。
これは簡便であるが、厳しすぎるものになっていたり、実際のデータにマッチしていなかったりする。

そのため、深層学習に動機づけて、グラフベースの深層学習モデルを導入することによる、信頼性の高いDAGの学習手法を開発した。
最終的に、変分推論の仕組みを利用して、GNNを用いたエンコーダ/デコーダのペアをパラメタ化している。
この場合、目的関数(スコア)は、変分下限(ELBO)になる。
現在の一般的なGNNのデザインとは異なり、線形のSEMを元に組まれており、データが線形で説明でできるなら、線形SEMと同様の性能を示すようなものになっている。

この手法は以下の特徴や利点を持っている。
1. VAEの仕組みを使ったデータ分布のキャプチャとサンプリング。
    グラフの設定としては、係数行列は潜在的な構造ではなく、明示的なパラメータになっており、ネットワークの学習と一緒に学習できる。
    こうしたネットワークの仕組みはこれまで使われていなかった。
2. VAEはさまざまなデータタイプに対応でき、連続値だけでなく離散的なものにも対応できる。
    必要なことは、変数の性質と一致する尤度分布（デコーダー出力）をモデル化することだけになっている。
3. パラメタ化にGNNを用いるため、各変数(ノード)はスカラー値だけでなくベクトル値を扱うこともできる。
    これらの変数はGNNに対する入力/出力の特徴量になる。
4. この手法は、非循環の要求をより現在の深層学習の仕組みに実装しやすいものにしている。
    NO TEARSでは、行列の指数関数の形式を用いていており、これは数学的にはとても美しいが、実装が難しかったり、自動微分ができなかったりする。
    そのため、これを多項式の形で近似した、より実用的に扱いやすく、指数関数と数式的に相当に扱える形にした。

デモンストレーションとして、線形/非線形のSEMに基づいた合成データを用いた試験、離散値のベンチマークデータを用いた試験、そしてアプリケーションからのテストデータセットを用いたものを行った。
合成データに対しては、DAG-NOTEARSよりもDAG-GNNが優れた性能を示している。
ベンチマークデータに対する結果としては、組み合わせ探索によるベイズを利用した探索手法と同等の結果を示した。


### 背景と関連研究

DAG(G)と結合分布(P)は、全ての条件付き独立性がPに対して真であることが、全てのGで確からしい時に、互いに信頼できる。  
信頼性のある状態では、結合分布PからGを復元できる。
分布やDAGが不明な状態で与えられた独立同分布(iid)サンプルDを使って、そのDAGを再現することが構造学習である。

データからDAGを学習するアルゴリズムは多く開発されてきており、スコアベースのものと制約ベースのものに大別される。

- スコアベースの手法は、ヒルクライムやフォワード-バックワード、ダイナミックプログラミング、整数プログラミングのように、データに対してどれだけそのグラフが合致しているかを示すようなスコアを用いて、最適なグラフを探すものである。
共通して用いる(BICのような)ベイズスコアは、分解可能であり、一貫性、局所的一貫性、スコア等価性を備えたものになっている。  
DAGの探索空間を扱える形にするために、近似手法が追加の仮定が課される(木構造の幅、構造など)。
多くのブートストラップやサンプリングベースの構造学習の手法でも、探索の課題に対しての解決を行なっている。
- 制約ベース
対照的に、エッジの存在を検証するための(状況下での)独立性のテストを使用している。
このような手法で有名なのは、SGS, PC, IC, FCIなどがある。  
近年では、MMHCのようなスコアベースとのハイブリッドな手法や、制約ベースの手法を複数の環境に対して行う手法なども現れている。

NP困難の問題のため、伝統的なDAGの学習においては、離散値か、ガウス分布になる変数を一般的には対象として扱う。
だが最近では、離散的な探索の問題を等式の条件に落とし込むような、連続値の最適化のアプローチも提案されている。
この方法によって、勾配降下法のような最適化手法が使えるようになった。
この手法は、線形のSEMに限ってにはなるが、構造の再現において良い結果を示すことに成功している。

ニューラルネットのアプローチは、近年始まったばかりである。
GANを使った手法では、個別の生成モデルがそれぞれの変数に適用され、実サンプルと生成サンプルの区別という形で選別が行われる。
このアプローチは、スケールとしては良いが、非循環の条件は強制されないという点がある。

### ニューラルネットでのDAG学習

この手法では、深層学習のモデルを使ってDAGの係数行列を学習する。
まずは起点として、線形のSEMに対するケースから話を始める。

#### 1. 線形SEM

実数空間$m$を因子数として、$A \in \vec{R}^{m \times m}$を重みを表現する係数行列として導入する。この時、入力データは$X \in \vec{R}^{m \times d}$ と表現し、$d$はデータの数とする。
(ここでは、変数は基本的にスカラー量であるとするが、一応形式上$d$次元のベクトルという形に一般化している。)  
この時、線形SEMは$Z$を誤差行列として、$X = A^{T}X + Z$という形で表現できる。  
グラフノードがトポロジカルな順番に並んでいれば、$A$は上三角形の形の三角行列になる。
したがって、DAGからのサンプリングは、乱数誤差$Z$を用いて以下のように生成するのと等価である: $X = (I - A^{T})^{-1} Z$

#### 2.GNNモデル

$X = (I - A^{T})^{-1} Z$を$X = f_{A}(Z)$と表現することにすると、これは深層学習の枠組みではよく用いられている、ノード$Z$を入力として$X$を返すような、GNNのパラメタ表現になっている。
例えば、よく用いられるGCNのモデルでは$X=\hat{A} \cdot ReLU(\hat{A}ZW^1) \cdot W^2$と読むことができる。
ここで、$\hat{A}$は$A$の規格化定数で、$W^1$と$W^2$はそれぞれのパラメータ行列である。  
ここで今の因果の特有な構造のため、新たなグラフ構成として$X=f_2((I-A^T)^{-1}f_1(Z))$というものを提案する。
この$f_1$と$f_2$は$Z$と$X$をそれぞれ効果的に変換するために導入されており、非線形にもなりうる。


#### 3. モデルの学習とVAE

![Knowledge Graph Categorize](img/VAE.png)

#### 4. 構造とロス関数

#### 5. 離散値

#### 6. 線形SEMへの結合

#### 7. 非線形の制約

#### 8. トレーニング

### 実験

### 結論
